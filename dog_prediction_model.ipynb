{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for garbage collection\n",
    "import gc\n",
    "\n",
    "# for warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# utility libraries\n",
    "import os\n",
    "import copy\n",
    "import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2, random, time, shutil, csv\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# keras libraries\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of classes read - 120\n"
     ]
    }
   ],
   "source": [
    "# set image size here\n",
    "img_size = 331\n",
    "data_dir = '/Users/mac/Documents/Personal Material/cv/project/dog-breed-prediction/'\n",
    "data_df = pd.read_csv(os.path.join(data_dir, 'labels.csv'))\n",
    "class_names = sorted(data_df['breed'].unique())\n",
    "print(f\"No. of classes read - {len(class_names)}\")\n",
    "#time.sleep(1)\n",
    "\n",
    "images_list = sorted(os.listdir(os.path.join(data_dir, 'train')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save class_names to a file\n",
    "with open('class_names.pkl', 'wb') as f:\n",
    "    pickle.dump(class_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boston_bull'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['id']==images_list[0].split('.')[0]].iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10222 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:23<00:00, 441.36it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "i = 0\n",
    "for image in tqdm.tqdm(images_list):\n",
    "    cls_name = data_df[data_df['id'] == image[:-4]].iloc[0,1]\n",
    "    cls_index = int(class_names.index(cls_name)) \n",
    "\n",
    "    # Reading RGB Images\n",
    "    image_path = os.path.join(data_dir, 'train',image)\n",
    "    orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    res_image = cv2.resize(orig_image,(img_size, img_size))\n",
    "    X.append(res_image)\n",
    "    Y.append(cls_index)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222 10222\n",
      "(10222, 331, 331, 3) (10222, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to arrays\n",
    "print(len(X), len(Y))\n",
    "Xarr = np.array(X)\n",
    "Yarr = np.array(Y).reshape(-1,1)\n",
    "\n",
    "del(X)\n",
    "print(Xarr.shape, Yarr.shape)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 331, 331, 3) (10222, 120)\n"
     ]
    }
   ],
   "source": [
    "# converting labels to one hot\n",
    "Yarr_hot = to_categorical(Y)\n",
    "print(Xarr.shape, Yarr_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION OF TRAINING ARRAYS\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "def get_features(model_name, data_preprocessor, data):\n",
    "    '''\n",
    "    1- Create a feature extractor to extract features from the data.\n",
    "    2- Returns the extracted features and the feature extractor.\n",
    "\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "\n",
    "    def preprocess(x):\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        x = tf.image.random_brightness(x, 0.5)\n",
    "        return x\n",
    "\n",
    "    ds = dataset.map(preprocess, num_parallel_calls=AUTO).batch(64)\n",
    "\n",
    "    input_size = data.shape[1:]\n",
    "    #Prepare pipeline.\n",
    "    input_layer = Input(input_size)\n",
    "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
    "\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                                input_shape=input_size)(preprocessor)\n",
    "\n",
    "    avg = GlobalAveragePooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
    "\n",
    "\n",
    "    #Extract feature.\n",
    "    feature_maps = feature_extractor.predict(ds, verbose=1)\n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    \n",
    "    # deleting variables\n",
    "    del(feature_extractor, base_model, preprocessor, dataset)\n",
    "    gc.collect()\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION OF VALIDAION AND TESTING ARRAYS\n",
    "def get_valfeatures(model_name, data_preprocessor, data):\n",
    "    '''\n",
    "    Same as above except not image augmentations applied.\n",
    "    Used for feature extraction of validation and testing.\n",
    "    '''\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "    ds = dataset.batch(64)\n",
    "\n",
    "    input_size = data.shape[1:]\n",
    "    #Prepare pipeline.\n",
    "    input_layer = Input(input_size)\n",
    "    preprocessor = Lambda(data_preprocessor)(input_layer)\n",
    "\n",
    "    base_model = model_name(weights='imagenet', include_top=False,\n",
    "                                input_shape=input_size)(preprocessor)\n",
    "\n",
    "    avg = GlobalAveragePooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs = input_layer, outputs = avg)\n",
    "    #Extract feature.\n",
    "    feature_maps = feature_extractor.predict(ds, verbose=1)\n",
    "    print('Feature maps shape: ', feature_maps.shape)\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETURNING CONCATENATED FEATURES USING MODELS AND PREPROCESSORS\n",
    "def get_concat_features(feat_func, models, preprocs, array):\n",
    "\n",
    "    print(f\"Beggining extraction with {feat_func.__name__}\\n\")\n",
    "    feats_list = []\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        \n",
    "        print(f\"\\nStarting feature extraction with {models[i].__name__} using {preprocs[i].__name__}\\n\")\n",
    "        # applying the above function and storing in list\n",
    "        feats_list.append(feat_func(models[i], preprocs[i], array))\n",
    "\n",
    "    # features concatenating\n",
    "    final_feats = np.concatenate(feats_list, axis=-1)\n",
    "    # memory saving\n",
    "    del(feats_list, array)\n",
    "    gc.collect()\n",
    "\n",
    "    return final_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING models and preprocessors imports \n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "inception_preprocessor = preprocess_input\n",
    "\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "xception_preprocessor = preprocess_input\n",
    "\n",
    "from keras.applications.nasnet import NASNetLarge, preprocess_input\n",
    "nasnet_preprocessor = preprocess_input\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "inc_resnet_preprocessor = preprocess_input\n",
    "\n",
    "models = [InceptionV3,  InceptionResNetV2, Xception, NASNetLarge]\n",
    "preprocs = [inception_preprocessor,  inc_resnet_preprocessor, \n",
    "            xception_preprocessor, nasnet_preprocessor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining extraction with get_features\n",
      "\n",
      "\n",
      "Starting feature extraction with InceptionV3 using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 1307s 8s/step\n",
      "Feature maps shape:  (10222, 2048)\n",
      "\n",
      "Starting feature extraction with InceptionResNetV2 using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 4703s 29s/step\n",
      "Feature maps shape:  (10222, 1536)\n",
      "\n",
      "Starting feature extraction with Xception using preprocess_input\n",
      "\n",
      "160/160 [==============================] - 1709s 11s/step\n",
      "Feature maps shape:  (10222, 2048)\n",
      "\n",
      "Starting feature extraction with NASNetLarge using preprocess_input\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
      "343610240/343610240 [==============================] - 47s 0us/step\n",
      "160/160 [==============================] - 4648s 29s/step\n",
      "Feature maps shape:  (10222, 4032)\n",
      "Final feature maps shape (10222, 9664)\n"
     ]
    }
   ],
   "source": [
    "# calculating features of the data\n",
    "\n",
    "final_train_features = get_concat_features(get_features, models, preprocs, Xarr)\n",
    "\n",
    "print('Final feature maps shape', final_train_features.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True,\n",
    "                                                   verbose=0)\n",
    "\n",
    "my_callback=[EarlyStop_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 1s 6ms/step - loss: 0.2072 - accuracy: 0.9357\n",
      "Saved model model_fold_1.h5 at models/model_fold_1.h5\n",
      "\n",
      "Starting fold 2\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 0.2331 - accuracy: 0.9296\n",
      "Saved model model_fold_2.h5 at models/model_fold_2.h5\n",
      "\n",
      "Starting fold 3\n",
      "\n",
      "Training...\n",
      "Evaluating model ...\n",
      "107/107 [==============================] - 0s 3ms/step - loss: 0.2463 - accuracy: 0.9375\n",
      "Saved model model_fold_3.h5 at models/model_fold_3.h5\n",
      "\n",
      " CV Score -\n",
      "\n",
      "Accuracy - 0.934259295463562\n",
      "\n",
      "Loss - 0.22888554632663727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=10).split(final_train_features, Y))\n",
    "\n",
    "trained_models = []\n",
    "accuracy = []\n",
    "losses = []\n",
    "\n",
    "#Prepare And Train DNN model\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits): \n",
    "\n",
    "    print(f\"\\nStarting fold {i+1}\\n\")\n",
    "    x_train_fold = final_train_features[train_idx, :]\n",
    "    y_train_fold = Yarr_hot[train_idx, :]\n",
    "    x_val_fold = final_train_features[valid_idx]\n",
    "    y_val_fold = Yarr_hot[valid_idx, :]\n",
    "\n",
    "    dnn = keras.models.Sequential([\n",
    "        InputLayer(final_train_features.shape[1:]),\n",
    "        Dropout(0.7),\n",
    "        Dense(120, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    dnn.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    print(\"Training...\")\n",
    "    #Train simple DNN on extracted features.\n",
    "    h = dnn.fit(x_train_fold, y_train_fold,\n",
    "                batch_size=128,\n",
    "                epochs=80,\n",
    "                verbose=0,\n",
    "                validation_data = (x_val_fold, y_val_fold),\n",
    "                callbacks=my_callback)  # max 95.07\n",
    "\n",
    "    print(\"Evaluating model ...\")\n",
    "    model_res = dnn.evaluate(x_val_fold, y_val_fold)\n",
    "\n",
    "    accuracy.append(model_res[1])\n",
    "    losses.append(model_res[0])\n",
    "    trained_models.append(dnn)\n",
    "\n",
    "    # Save the trained models\n",
    "    model_name = f'model_fold_{i+1}.h5'\n",
    "    model_path = os.path.join('models', model_name)\n",
    "    dnn.save(model_path)\n",
    "    print(f\"Saved model {model_name} at {model_path}\")\n",
    "\n",
    "print('\\n CV Score -')\n",
    "print(f\"\\nAccuracy - {sum(accuracy)/len(accuracy)}\")\n",
    "print(f\"\\nLoss - {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained models\n",
    "for i, model in enumerate(trained_models):\n",
    "    model_name = f'trained_model_fold_{i+1}.h5'\n",
    "    model.save(os.path.join('models', model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of model names\n",
    "model_names = [f'model_fold_{i+1}.h5' for i in range(len(trained_models))]\n",
    "with open('model_names.txt', 'w') as file:\n",
    "    file.write('\\n'.join(model_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4527"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SAVING RAM\n",
    "\n",
    "del(final_train_features, Y, Yarr_hot, Xarr)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:16<00:00, 642.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10357, 331, 331, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST IMAGES\n",
    "test_images_list = sorted(os.listdir(os.path.join(data_dir, 'test')))\n",
    "X = []\n",
    "i = 0\n",
    "for image in tqdm.tqdm(test_images_list):\n",
    "\n",
    "    image_path = os.path.join(data_dir, 'test',image)\n",
    "    orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    res_image = cv2.resize(orig_image,(img_size, img_size))\n",
    "    X.append(res_image)\n",
    "    i+=1\n",
    "\n",
    "Xtesarr = np.array(X)\n",
    "\n",
    "del(X)\n",
    "gc.collect()\n",
    "\n",
    "Xtesarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining extraction with get_valfeatures\n",
      "\n",
      "\n",
      "Starting feature extraction with InceptionV3 using preprocess_input\n",
      "\n",
      "162/162 [==============================] - 1321s 8s/step\n",
      "Feature maps shape:  (10357, 2048)\n",
      "\n",
      "Starting feature extraction with InceptionResNetV2 using preprocess_input\n",
      "\n",
      "162/162 [==============================] - 2714s 17s/step\n",
      "Feature maps shape:  (10357, 1536)\n",
      "\n",
      "Starting feature extraction with Xception using preprocess_input\n",
      "\n",
      "162/162 [==============================] - 2968s 13s/step\n",
      "Feature maps shape:  (10357, 2048)\n",
      "Final feature maps shape (10357, 5632)\n"
     ]
    }
   ],
   "source": [
    "# FEATURE EXTRACTION OF TEST IMAGES\n",
    "test_features = get_concat_features(get_valfeatures, models, preprocs, Xtesarr)\n",
    "\n",
    "del(Xtesarr)\n",
    "gc.collect()\n",
    "print('Final feature maps shape', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step\n",
      "81/81 [==============================] - 0s 2ms/step\n",
      "81/81 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10357, 120)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_norm = trained_models[0].predict(test_features, batch_size=128)/3\n",
    "for dnn in trained_models[1:]:\n",
    "    y_pred_norm += dnn.predict(test_features, batch_size=128)/3\n",
    "\n",
    "y_pred_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(test_images_list) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_images_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Select a random test image index\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m random_index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(test_images_list) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Retrieve the corresponding test image and its predicted probabilities\u001b[39;00m\n\u001b[1;32m      7\u001b[0m image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_dir, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, test_images_list[random_index])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_images_list' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select a random test image index\n",
    "random_index = random.randint(0, len(test_images_list) - 1)\n",
    "\n",
    "# Retrieve the corresponding test image and its predicted probabilities\n",
    "image_path = os.path.join(data_dir, 'test', test_images_list[random_index])\n",
    "\n",
    "predicted_probs = y_pred_norm[random_index]\n",
    "\n",
    "# Get the predicted breed label\n",
    "predicted_label_index = np.argmax(predicted_probs)\n",
    "predicted_label = class_names[predicted_label_index]\n",
    "\n",
    "# Display the image\n",
    "image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Predicted Label: {predicted_label}')\n",
    "plt.show()\n",
    "\n",
    "# Plot the predicted probabilities\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_names, predicted_probs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Dog Breed')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Predicted Probabilities')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, breed]\n",
       "Index: []"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['id'] == 'bb0c7d7af4bdc0d3646afaf1339a15f2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10222 entries, 0 to 10221\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      10222 non-null  object\n",
      " 1   breed   10222 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 159.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble learning is a machine learning technique that combines multiple individual models to create a more powerful and accurate model. The idea behind ensemble learning is that by combining the predictions of multiple models, you can leverage the strengths and compensate for the weaknesses of each individual model, leading to improved overall performance.\n",
    "\n",
    "In your code, you train multiple models using different architectures (InceptionV3, InceptionResNetV2, Xception), and then you concatenate the extracted features from these models into a single feature vector. This concatenated feature vector is used as input to a final Dense layer for classification. During prediction, the outputs of the trained models are combined by taking an average of their predictions.\n",
    "\n",
    "By combining multiple models, each with its own unique approach or architecture, ensemble learning can help improve the accuracy and generalization of the final model. It can also enhance the model's ability to handle different types of inputs or capture diverse patterns in the data.\n",
    "\n",
    "Ensemble learning is a powerful technique that has been widely used in various machine learning tasks, including classification, regression, and anomaly detection. It offers a way to leverage the benefits of different models and improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/mac/Desktop/gettyimages-589656325-1-1586896598.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m orig_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mimread(image_path), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m      7\u001b[0m res_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(orig_image,(img_size, img_size))\n\u001b[1;32m      8\u001b[0m X\u001b[39m.\u001b[39mappend(res_image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "#Test on new image\n",
    "X = []\n",
    "i = 0\n",
    "\n",
    "image_path = '/Users/mac/Desktop/German-Shepherd-dog-Alsatian.jpg'\n",
    "orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "res_image = cv2.resize(orig_image,(img_size, img_size))\n",
    "X.append(res_image)\n",
    "\n",
    "Xtesarr = np.array(X)\n",
    "Xtesarr.shape\n",
    "\n",
    "del(X)\n",
    "gc.collect()\n",
    "# FEATURE EXTRACTION OF TEST IMAGES\n",
    "test_features = get_concat_features(get_valfeatures, models, preprocs, Xtesarr)\n",
    "\n",
    "y_pred_norm = trained_models[0].predict(test_features, batch_size=128)/3\n",
    "for dnn in trained_models[1:]:\n",
    "    y_pred_norm += dnn.predict(test_features, batch_size=128)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted_probs \u001b[39m=\u001b[39m y_pred_norm[\u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[39m# Get the predicted breed label\u001b[39;00m\n\u001b[1;32m      4\u001b[0m predicted_label_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predicted_probs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_norm' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_probs = y_pred_norm[0]\n",
    "\n",
    "# Get the predicted breed label\n",
    "predicted_label_index = np.argmax(predicted_probs)\n",
    "predicted_label = class_names[predicted_label_index]\n",
    "\n",
    "# Display the image\n",
    "image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title(f'Predicted Label: {predicted_label}')\n",
    "plt.show()\n",
    "\n",
    "# Plot the predicted probabilities\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_names, predicted_probs)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Dog Breed')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Predicted Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    X = []\n",
    "    orig_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    res_image = cv2.resize(orig_image, (img_size, img_size))\n",
    "    X.append(res_image)\n",
    "    X_test = np.array(X)\n",
    "\n",
    "    test_features = get_concat_features(get_valfeatures, models, preprocs, X_test)\n",
    "    \n",
    "\n",
    "    y_pred_norm = trained_models[0].predict(test_features, batch_size=128) / 3\n",
    "    for dnn in trained_models[1:]:\n",
    "        y_pred_norm += dnn.predict(test_features, batch_size=128) / 3\n",
    "    predicted_probs = y_pred_norm[0]\n",
    "\n",
    "    # Get the predicted breed label\n",
    "    predicted_label_index = np.argmax(predicted_probs)\n",
    "    predicted_label = class_names[predicted_label_index]\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beggining extraction with get_valfeatures\n",
      "\n",
      "\n",
      "Starting feature extraction with InceptionV3 using preprocess_input\n",
      "\n",
      "1/1 [==============================] - 1s 987ms/step\n",
      "Feature maps shape:  (1, 2048)\n",
      "\n",
      "Starting feature extraction with InceptionResNetV2 using preprocess_input\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Feature maps shape:  (1, 1536)\n",
      "\n",
      "Starting feature extraction with Xception using preprocess_input\n",
      "\n",
      "1/1 [==============================] - 1s 864ms/step\n",
      "Feature maps shape:  (1, 2048)\n",
      "\n",
      "Starting feature extraction with NASNetLarge using preprocess_input\n",
      "\n",
      "WARNING:tensorflow:5 out of the last 164 calls to <function Model.make_predict_function.<locals>.predict_function at 0x15317ab60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "Feature maps shape:  (1, 4032)\n",
      "WARNING:tensorflow:6 out of the last 165 calls to <function Model.make_predict_function.<locals>.predict_function at 0x12e88c360> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'german_shepherd'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_image('/Users/mac/Desktop/German-Shepherd-dog-Alsatian.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
